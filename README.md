# Sign language Detection

This project aims at extending a step forward in this field by collecting a dataset and then use various feature extraction techniques to extract useful information which is then input into various supervised learning techniques. To more easily approach the problem and obtain reasonable results, we experimented with just up to 5 different classes/words in our self-made dataset instead of all 26 possible letters.

We achieved a classification accuracy of 97% on a randomly selected set of test data using our trained model. In addition to the work we did on static images, we also created a live demo version of the project which can be run at a little less than 2 seconds per frame to classify signed hand gestures from any person.

## Collecting dataset
![image](https://user-images.githubusercontent.com/74018041/121787111-e3829c80-cbe1-11eb-9353-f6c4ca2df4b6.png)

## Testing the Model
![image](https://user-images.githubusercontent.com/74018041/121787128-090fa600-cbe2-11eb-8a24-976dc1c79d94.png)



Result for HELLO 
![image](https://user-images.githubusercontent.com/74018041/121787208-6efc2d80-cbe2-11eb-9e0c-16067929a88f.png) 
Result for BYE
![image](https://user-images.githubusercontent.com/74018041/121787219-7cb1b300-cbe2-11eb-8d61-0528aafeef17.png)

Result for NO 
![image](https://user-images.githubusercontent.com/74018041/121787263-b71b5000-cbe2-11eb-93a6-33205e41efeb.png)
Result for OKAY
![image](https://user-images.githubusercontent.com/74018041/121787270-c0a4b800-cbe2-11eb-99b6-4750c0c56c10.png)
  

### Time Taken to detect = 0.922s & Accuracy of our model = 97%

